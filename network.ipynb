{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import csv\n",
    "from dragon import dragonV\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = 'C:/Users/admin/Desktop/Philadelphia/real_final_data/'\n",
    "subject_list = ['pp01', 'pp02', 'pp009', 'pp085', 'pp086', 'pp087', 'pp088', 'pp089'] \n",
    "#front, rear 2가지 존재. 파일명 format : {gait_catergory}_{front || rear}.xlsx\n",
    "gait_category_list = ['gait1', 'gait2', 'fast', 'preferred', 'reaction', 'slow', 'stroop', 'turn']\n",
    "seleted_openpose_joint_idx_list = [8, 9, 10, 11, 12, 13, 14, 19, 20, 21, 22, 23, 24]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xlsx -> nomalize -> lower joint -> frame & gt pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subject_data = []\n",
    "for each_subject in subject_list:\n",
    "    #root/ppxx/\n",
    "    each_subject_path = root_path + each_subject + '/'\n",
    "    files = os.listdir(each_subject_path)\n",
    "\n",
    "    ###xlsx_list###\n",
    "    excel_name_list = [file for file in files if file.endswith('.xlsx')]\n",
    "    \n",
    "    ###gt_list####\n",
    "    tmp = os.listdir(each_subject_path + 'gt/')\n",
    "    gt_name_list = [file for file in tmp if file.endswith('.npy')]\n",
    "\n",
    "    #ppxx/{each_excel_name}\n",
    "    #하나의 엑셀 파일에 대한 반복문\n",
    "    for each_excel in excel_name_list:\n",
    "        ##xlsx -> list##\n",
    "        frame_data_list = dragonV.xlsx2data(each_subject_path + each_excel)\n",
    "        #print(f'{each_excel} : {len(frame_data_list[0])}')\n",
    "        ##list -> normalize##\n",
    "        norm_frame_data_list = dragonV.nomalize_data(frame_data_list)\n",
    "        #print(f'{each_subject}->{each_excel} : {len(norm_frame_data_list[0])}')\n",
    "\n",
    "        ##noramlize -> select lower joint pos##\n",
    "        selected_norm_frame_data_list = dragonV.get_selected_joint_pos_frame_list(norm_frame_data_list, seleted_openpose_joint_idx_list)\n",
    "        #print(f'{each_subject}->{each_excel} : {len(selected_norm_frame_data_list[0])}')\n",
    "        \n",
    "        ## 여기까진 왔음...\n",
    "        ##frame data : gt pair##\n",
    "        for each_gt in gt_name_list:\n",
    "            #확장자명 제거\n",
    "            each_gt_name = each_gt[:-4]\n",
    "            if each_gt_name in each_excel:\n",
    "                #print(f'{each_subject}:{each_excel} -> {each_gt_name}')\n",
    "                right_gt_np = np.load(each_subject_path + '/gt/' + each_gt)\n",
    "                right_gt_list = right_gt_np.tolist()\n",
    "\n",
    "                data_gt_pair_list = dragonV.make_dataAndGtPair(selected_norm_frame_data_list, right_gt_list)\n",
    "                all_subject_data.append(data_gt_pair_list)\n",
    "            else:\n",
    "                continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = len(all_subject_data)\n",
    "d2 = len(all_subject_data[0])\n",
    "d3 = len(all_subject_data[0][0])\n",
    "d4_0 = len(all_subject_data[0][0][0])\n",
    "d4_1 = len(all_subject_data[0][0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_num : 73\n",
      "frame_num : 253\n",
      "features || label : 2\n",
      "d4_0 : 26\n",
      "d4_1 : 4\n"
     ]
    }
   ],
   "source": [
    "print('video_num :',d1)\n",
    "print('frame_num :',d2)\n",
    "print('features || label :',d3)\n",
    "print('d4_0 :',d4_0)\n",
    "print('d4_1 :',d4_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list -> torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98548271, 0.92060491, 0.98759102, 0.96146204, 0.9881226 ,\n",
       "       0.9740866 , 0.99484523, 0.98557527, 0.81086829, 0.91704873,\n",
       "       0.70520791, 0.97594994, 0.71989963, 0.9997064 , 0.62189694,\n",
       "       0.89097924, 0.9188043 , 0.9825343 , 0.73717281, 0.98529396,\n",
       "       0.99225403, 0.97184447, 0.98237504, 0.9717276 , 0.99244709,\n",
       "       0.97882186])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_num = 0\n",
    "frame_num = 0\n",
    "features_labels = 0\n",
    "np.array(all_subject_data[video_num][frame_num][features_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[\n",
    "    [\n",
    "        [features],\n",
    "        [features],\n",
    "        ......... ,\n",
    "        [features]\n",
    "    ],\n",
    "    [\n",
    "        [features],\n",
    "        [features],\n",
    "        ......... ,\n",
    "        [features]\n",
    "    ],\n",
    "    [\n",
    "        [features],\n",
    "        [features],\n",
    "        ......... ,\n",
    "        [features]\n",
    "    ],    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "163\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "print(len(all_subject_data))\n",
    "print(len(all_subject_data[0]))\n",
    "print(len(all_subject_data[0][0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timesteps이 9라서 앞, 뒤로 레이블 데이터 4개씩 버려야 함.\n",
    "'''\n",
    "each_video : frame_num x (features || label) x (26 || 4)\n",
    "'''\n",
    "trimmed_data = []\n",
    "trimmed_label = []\n",
    "\n",
    "for each_video in all_subject_data:\n",
    "    #각 비디오의 전체 프레임 길이\n",
    "    video_len = len(each_video)\n",
    "    #frame_number x each_element(26)\n",
    "    total_frame_features_list = []\n",
    "    #frame_number x each_label(4)\n",
    "    total_frame_label_list = []\n",
    "    \n",
    "    #각각의 비디오에서 frame_data와 frame별 label 값 추출\n",
    "    for i in range(0, video_len):\n",
    "        total_frame_features_list.append(each_video[i][0])\n",
    "        total_frame_label_list.append(each_video[i][1])\n",
    "\n",
    "    #timesteps 수만큼 frame data 묶기.\n",
    "    for idx in range(0, len(total_frame_features_list) - 8):\n",
    "        trimmed_data.append(total_frame_features_list[idx:idx+9])\n",
    "    \n",
    "    del total_frame_label_list[0:4]\n",
    "    del total_frame_label_list[-4:]\n",
    "    \n",
    "    for label in total_frame_label_list:\n",
    "        trimmed_label.append(label)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_num : 45834\n",
      "timesteps : 9\n",
      "input_size : 26\n",
      "\n",
      "\n",
      "sample_num : 45834\n",
      "label_size : 4\n"
     ]
    }
   ],
   "source": [
    "####data info###\n",
    "print(f'sample_num : {len(trimmed_data)}')\n",
    "print(f'timesteps : {len(trimmed_data[0])}')\n",
    "print(f'input_size : {len(trimmed_data[0][0])}')\n",
    "##########\n",
    "\n",
    "print('\\n')\n",
    "###label###\n",
    "print(f'sample_num : {len(trimmed_label)}')\n",
    "print(f'label_size : {len(trimmed_label[0])}')\n",
    "\n",
    "########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# data 저장된 곳\\ntrimmed_data : list\\ntrimmed_label : list\\n\\ntrimmed_data : sample_size x timesteps(9) x input_size(26)\\ntrimmed_label : sample_size x label_size(4)\\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# data 저장된 곳\n",
    "trimmed_data : list\n",
    "trimmed_label : list\n",
    "\n",
    "trimmed_data : sample_size x timesteps(9) x input_size(26)\n",
    "trimmed_label : sample_size x label_size(4)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "outputs, _status = cell(output)<br>\n",
    "\n",
    "(모든 timesteps에 대한 결과)<br>\n",
    "outputs -> [bactch x timesteps x output]<br><br>\n",
    "(마지막 timestpes에 대한 결과)<br>\n",
    "_status -> [1 x batch x output]<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data to tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_np = np.array(trimmed_data)\n",
    "label_data_np = np.array(trimmed_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45834\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(len(trimmed_data))\n",
    "print(len(trimmed_data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_np = np.load('C:/Users/admin/Desktop/Philadelphia/input_data.npy')\n",
    "label_data_np = np.load('C:/Users/admin/Desktop/Philadelphia/label_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_tensor = torch.tensor(input_data_np, dtype=torch.float32)\n",
    "label_tensor = torch.tensor(label_data_np, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN 값이 있는지 여부: False\n"
     ]
    }
   ],
   "source": [
    "has_nan = torch.isnan(input_data_tensor).any().item()\n",
    "print(\"NaN 값이 있는지 여부:\", has_nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN 값이 있는 인덱스: tensor([[29822,     8,     0],\n",
      "        [29822,     8,     1],\n",
      "        [29822,     8,     2],\n",
      "        ...,\n",
      "        [31557,     0,    23],\n",
      "        [31557,     0,    24],\n",
      "        [31557,     0,    25]])\n"
     ]
    }
   ],
   "source": [
    "# NaN 값을 포함한 인덱스 확인\n",
    "nan_indices = torch.isnan(input_data_tensor).nonzero()\n",
    "print(\"NaN 값이 있는 인덱스:\", nan_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaN 값을 평균 값으로 대체하는 함수\n",
    "def replace_nan_with_mean(tensor):\n",
    "    # NaN 값을 평균 값으로 대체하기 위해 평균 값을 계산\n",
    "    mean_values = torch.nanmean(tensor, dim=(0, 1, 2))\n",
    "    # NaN 값을 대체할 인덱스를 찾음\n",
    "    nan_indices = torch.isnan(tensor)\n",
    "    # NaN 값을 평균 값으로 대체\n",
    "    tensor[nan_indices] = mean_values\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_tensor = replace_nan_with_mean(input_data_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([45834, 9, 26])\n",
      "torch.Size([45834, 4])\n"
     ]
    }
   ],
   "source": [
    "print(input_data_tensor.shape)\n",
    "print(label_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 26\n",
    "output_size = 4\n",
    "hidden_size = 128\n",
    "batch_size = 10\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_data_tensor, label_tensor)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "Data shape: torch.float32\n",
      "Target shape: torch.int64\n"
     ]
    }
   ],
   "source": [
    "#DataLoader 테스트\n",
    "for batch_idx, (data, target) in enumerate(data_loader):\n",
    "    print(\"Batch\", batch_idx)\n",
    "    print(\"Data shape:\", data.dtype)  # 미니배치의 입력 데이터 모양\n",
    "    print(\"Target shape:\", target.dtype)\n",
    "\n",
    "    if batch_idx == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## network 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        out = self.fc(hidden[-1])\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## criterian && optimizer 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(input_size, hidden_size, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 9, 26])\n",
      "tensor([[0.4951, 0.5164, 0.4725, 0.5228],\n",
      "        [0.4976, 0.5153, 0.4725, 0.5212],\n",
      "        [0.4948, 0.5170, 0.4718, 0.5238],\n",
      "        [0.4971, 0.5152, 0.4737, 0.5208],\n",
      "        [0.4920, 0.5171, 0.4745, 0.5208],\n",
      "        [0.5041, 0.5109, 0.4808, 0.5107],\n",
      "        [0.4955, 0.5164, 0.4730, 0.5223],\n",
      "        [0.4943, 0.5172, 0.4712, 0.5242],\n",
      "        [0.4955, 0.5164, 0.4724, 0.5224],\n",
      "        [0.4968, 0.5154, 0.4725, 0.5223]], grad_fn=<SigmoidBackward0>)\n",
      "tensor([[0, 0, 1, 1],\n",
      "        [0, 1, 0, 0],\n",
      "        [1, 1, 1, 1],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [1, 1, 0, 0],\n",
      "        [1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (data, target) in enumerate(data_loader):\n",
    "\n",
    "    print(data.shape)\n",
    "    outputs = model(data)\n",
    "    print(outputs)\n",
    "    print(target)\n",
    "    loss = criterion(outputs, target.float())\n",
    "    if batch_idx == 0:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, data_loader, num_epochs, criterion, optimizer):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            outputs = model(inputs)\n",
    "            #print(outputs)\n",
    "            loss = criterion(outputs, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # 예측값 계산\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            total_predictions += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_loss = running_loss / len(data_loader.dataset)\n",
    "        if(epoch % 10 == 0):\n",
    "            print(f'Epoch [{epoch}/{num_epochs}], Loss: {epoch_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 0.0560\n",
      "Epoch [20/1000], Loss: 0.0485\n",
      "Epoch [30/1000], Loss: 0.0434\n",
      "Epoch [40/1000], Loss: 0.0402\n",
      "Epoch [50/1000], Loss: 0.0376\n",
      "Epoch [60/1000], Loss: 0.0355\n",
      "Epoch [70/1000], Loss: 0.0333\n",
      "Epoch [80/1000], Loss: 0.0316\n",
      "Epoch [90/1000], Loss: 0.0300\n",
      "Epoch [100/1000], Loss: 0.0290\n",
      "Epoch [110/1000], Loss: 0.0278\n",
      "Epoch [120/1000], Loss: 0.0271\n",
      "Epoch [130/1000], Loss: 0.0262\n",
      "Epoch [140/1000], Loss: 0.0255\n",
      "Epoch [150/1000], Loss: 0.0248\n",
      "Epoch [160/1000], Loss: 0.0249\n",
      "Epoch [170/1000], Loss: 0.0236\n",
      "Epoch [180/1000], Loss: 0.0230\n",
      "Epoch [190/1000], Loss: 0.0226\n",
      "Epoch [200/1000], Loss: 0.0223\n",
      "Epoch [210/1000], Loss: 0.0219\n",
      "Epoch [220/1000], Loss: 0.0215\n",
      "Epoch [230/1000], Loss: 0.0208\n",
      "Epoch [240/1000], Loss: 0.0209\n",
      "Epoch [250/1000], Loss: 0.0202\n",
      "Epoch [260/1000], Loss: 0.0204\n",
      "Epoch [270/1000], Loss: 0.0200\n",
      "Epoch [280/1000], Loss: 0.0198\n",
      "Epoch [290/1000], Loss: 0.0193\n",
      "Epoch [300/1000], Loss: 0.0192\n",
      "Epoch [310/1000], Loss: 0.0189\n",
      "Epoch [320/1000], Loss: 0.0189\n",
      "Epoch [330/1000], Loss: 0.0186\n",
      "Epoch [340/1000], Loss: 0.0182\n",
      "Epoch [350/1000], Loss: 0.0181\n",
      "Epoch [360/1000], Loss: 0.0182\n",
      "Epoch [370/1000], Loss: 0.0181\n",
      "Epoch [380/1000], Loss: 0.0178\n",
      "Epoch [390/1000], Loss: 0.0179\n",
      "Epoch [400/1000], Loss: 0.0178\n",
      "Epoch [410/1000], Loss: 0.0173\n",
      "Epoch [420/1000], Loss: 0.0173\n",
      "Epoch [430/1000], Loss: 0.0173\n",
      "Epoch [440/1000], Loss: 0.0174\n",
      "Epoch [450/1000], Loss: 0.0169\n",
      "Epoch [460/1000], Loss: 0.0167\n",
      "Epoch [470/1000], Loss: 0.0169\n",
      "Epoch [480/1000], Loss: 0.0170\n",
      "Epoch [490/1000], Loss: 0.0165\n",
      "Epoch [500/1000], Loss: 0.0165\n",
      "Epoch [510/1000], Loss: 0.0164\n",
      "Epoch [520/1000], Loss: 0.0159\n",
      "Epoch [530/1000], Loss: 0.0165\n",
      "Epoch [540/1000], Loss: 0.0161\n",
      "Epoch [550/1000], Loss: 0.0161\n",
      "Epoch [560/1000], Loss: 0.0160\n",
      "Epoch [570/1000], Loss: 0.0159\n",
      "Epoch [580/1000], Loss: 0.0157\n",
      "Epoch [590/1000], Loss: 0.0163\n",
      "Epoch [600/1000], Loss: 0.0160\n",
      "Epoch [610/1000], Loss: 0.0154\n",
      "Epoch [620/1000], Loss: 0.0156\n",
      "Epoch [630/1000], Loss: 0.0154\n",
      "Epoch [640/1000], Loss: 0.0152\n",
      "Epoch [650/1000], Loss: 0.0150\n",
      "Epoch [660/1000], Loss: 0.0152\n",
      "Epoch [670/1000], Loss: 0.0154\n",
      "Epoch [680/1000], Loss: 0.0150\n",
      "Epoch [690/1000], Loss: 0.0149\n",
      "Epoch [700/1000], Loss: 0.0156\n",
      "Epoch [710/1000], Loss: 0.0152\n",
      "Epoch [720/1000], Loss: 0.0152\n",
      "Epoch [730/1000], Loss: 0.0151\n",
      "Epoch [740/1000], Loss: 0.0149\n",
      "Epoch [750/1000], Loss: 0.0147\n",
      "Epoch [760/1000], Loss: 0.0157\n",
      "Epoch [770/1000], Loss: 0.0141\n",
      "Epoch [780/1000], Loss: 0.0146\n",
      "Epoch [790/1000], Loss: 0.0144\n",
      "Epoch [800/1000], Loss: 0.0144\n",
      "Epoch [810/1000], Loss: 0.0152\n",
      "Epoch [820/1000], Loss: 0.0142\n",
      "Epoch [830/1000], Loss: 0.0146\n",
      "Epoch [840/1000], Loss: 0.0164\n",
      "Epoch [850/1000], Loss: 0.0147\n",
      "Epoch [860/1000], Loss: 0.0143\n",
      "Epoch [870/1000], Loss: 0.0142\n",
      "Epoch [880/1000], Loss: 0.0144\n",
      "Epoch [890/1000], Loss: 0.0138\n",
      "Epoch [900/1000], Loss: 0.0141\n",
      "Epoch [910/1000], Loss: 0.0139\n",
      "Epoch [920/1000], Loss: 0.0135\n",
      "Epoch [930/1000], Loss: 0.0147\n",
      "Epoch [940/1000], Loss: 0.0138\n",
      "Epoch [950/1000], Loss: 0.0142\n",
      "Epoch [960/1000], Loss: 0.0135\n",
      "Epoch [970/1000], Loss: 0.0133\n",
      "Epoch [980/1000], Loss: 0.0139\n",
      "Epoch [990/1000], Loss: 0.0133\n",
      "Epoch [1000/1000], Loss: 0.0135\n"
     ]
    }
   ],
   "source": [
    "train_network(model, data_loader, num_epochs=num_epochs, criterion=criterion, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../Philadelphia/models/1000_epochs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.0696\n",
      "Epoch [20/100], Loss: 0.0696\n",
      "Epoch [30/100], Loss: 0.0696\n",
      "Epoch [40/100], Loss: 0.0696\n",
      "Epoch [50/100], Loss: 0.0696\n",
      "Epoch [60/100], Loss: 0.0696\n",
      "Epoch [70/100], Loss: 0.0696\n",
      "Epoch [80/100], Loss: 0.0696\n",
      "Epoch [90/100], Loss: 0.0696\n",
      "Epoch [100/100], Loss: 0.0696\n"
     ]
    }
   ],
   "source": [
    "model_100 = LSTM(input_size, hidden_size, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "train_network(model_100, data_loader, num_epochs=100, criterion=criterion, optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '../Philadelphia/models/100_epochs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, criterion, threshold):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "\n",
    "    accuracy = 0.0\n",
    "    precision = 0.0\n",
    "    recall = 0.0\n",
    "    f1 = 0.0\n",
    "\n",
    "    test_loss = 0\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "\n",
    "\n",
    "    # 다중 레이블 분류 평가 지표 계산, macro : 각 클래스에 대해 개별적으로 평가\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision = precision_score(true_labels, predictions, average='macro')\n",
    "    recall = recall_score(true_labels, predictions, average='macro')\n",
    "    f1 = f1_score(true_labels, predictions, average='macro')\n",
    "\n",
    "    print(f'Average Test Loss: {avg_loss:.4f}')\n",
    "    print(f'Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
